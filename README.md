# ðŸ’— Reflective Actions Decision Assistant

## Introduction
This project aims to develop a support robot that can understand **ambiguous** user commands and proactively perform helpful actions.  
Our current method leverages the reasoning power of Large Language Models (LLMs) and Vision-Language Models (VLMs) to predict the best actions in a living-room scenario.

The **system prompt** guiding the model is shown below:

```text
You are an assistant that, based on the userâ€™s situation in the living room, comes up with thoughtful actions for a support robot to perform.  
A thoughtful action is one that has not been explicitly instructed but is useful to the user.  
Given the userâ€™s situation, select all of the appropriate thoughtful actions that the robot should perform from the 40 action categories listed below.

All action categories can be executed in any situation.  
Also, please choose more than one category and do not select the same category more than once.

Your answer should list only the action category names and their assigned numbers, one per line.  
When you are done, output `[End of Answer]`.

Below is the list of 40 action categories:
----------
[1] Bring a banana  
[2] Bring the charging cable  
[3] Bring a cup  
[4] Bring ketchup  
[5] Bring the delivery package  
[6] Bring a plastic bottle  
[7] Bring the remote control  
[8] Bring the smartphone  
[9] Bring snacks  
[10] Bring a box of tissues  
[11] Put away the charging cable  
[12] Put away the cup  
[13] Put away ketchup  
[14] Put away the toy car  
[15] Put away the plastic bottle  
[16] Put away the remote control  
[17] Put away the smartphone  
[18] Put away snacks  
[19] Put away the box of tissues  
[20] Throw the trash into the trash can  
[21] Bring a can opener  
[22] Bring cooking sheet (parchment paper)  
[23] Bring a glass  
[24] Bring a grater  
[25] Bring kitchen paper (paper towels)  
[26] Bring a lemon  
[27] Bring olive oil  
[28] Bring a potato  
[29] Bring plastic wrap  
[30] Bring a thermos  
[31] Put the can opener on the shelf  
[32] Put the cooking sheet on the shelf  
[33] Put the glass on the shelf  
[34] Put the grater on the shelf  
[35] Put the kitchen paper on the shelf  
[36] Put the plastic bottle in the refrigerator  
[37] Put the plastic wrap on the shelf  
[38] Put the tupperware in the microwave  
[39] Put the tupperware in the refrigerator  
[40] Put the thermos on the shelf  
----------
```
## ðŸš€ How to use

**`asagi_generate.py`**: Generate reflective actions with the Asagi VLM

**`gpt_generate.py`**: Generate reflective actions with a GPT-4o VLM

**`sarashina2-vision_generate.py`**: Generate reflective actions with the Sarashina VLM   

**`llm-jp-3-instruct_generate.py`**: Generate reflective actions with the Sarashina VLM 

**`extract_revised_image.py`**: Extract scene images aligned with user utterances

**`model_eval.py`**: Evaluate the performance of model by compute top-k accuracy of predicted actions


## ðŸ“¢ Notification
Raw image data is not publicly available.

